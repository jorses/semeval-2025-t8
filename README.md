# SemEval 2025 Task 8: Task Paper

This repo contains the (pre-camera ready) version of our Task 8 paper in case participants wish to cite it
or change something for their camera-ready versions.

Please cite it using this
```BiBTeX
@InProceedings{ossgrijalba-EtAl:2025:SemEval2025,
  author    = "Os\'es Grijalba, Jorge  and  Ure√±a-L\'opez, L. Alfonso  and  Mart\'inez C\'amara, Eugenio  and  Camacho-Collados, Jose",
  title     = "SemEval-2025 Task 8: Question Answering over Tabular Data",
  booktitle      = "Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025)",
  month          = "August",
  year           = "2025",
  address        = "Vienna, Austria",
  publisher      = "Association for Computational Linguistics",
  pages     = "1015--1022",
  abstract  = "We introduce the findings and results of SemEval-2025 Task 8: Question Answering over Tabular Data. We featured two subtasks, DataBench and DataBench Lite. DataBench consists on question answering over tabular data, and DataBench Lite small comprising small datasets that might be easier to manage by current models by for example fitting them into a prompt. The task was open for any approach, but their answer has to conform to a required typing format. In this paper we present the task, analyze a number of system submissions and discuss the results. The results show how approaches leveraging LLMs dominated the task, with larger models exhibiting a considerably superior performance compared to small models.",
  url       = "https://aclanthology.org/2025.semeval2025-1.135"
}
```
